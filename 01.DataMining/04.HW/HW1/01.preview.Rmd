---
title: "R Notebook"
output:
  prettydoc::html_pretty:
    highlight: github
    theme: architect
  html_document: default
  html_notebook: default
  github_document: default
---
```{r echo=FALSE}
# include this code chunk as-is to set options
knitr::opts_chunk$set(comment=NA, prompt=FALSE, out.width=750, fig.height=8, fig.width=8)
```

# About our data
- gene information telling how many copy one has. Each informations are drawn from 6 probes which are denoted as variable P4075, P4076, P4077, P4078, P4079, P4080. 
- Data is observed from 408 genes(people?). 


# 1. Let's check how our data looks like(linearity, normality etc.)


#### We have 6 variables, and there seems to be 2 missing values(NA).
```{r}
cnp6 <- read.csv("./cnp6.csv")
head(cnp6)
summary(cnp6)
str(cnp6)
```

#### Let's remove missing values
```{r}

# 결측치가 몇 개 있는지 함수로 구한다. 
counting.missing.values <- function(x){
  count.na <- rep(NA,length(x)-1)
  for(i in 1:length(x)-1){
    count.na[i] <- sum(is.na(x[,i+1]))
  }
  print(count.na)
}

counting.missing.values(cnp6)

# 결측치를 제거하자. 
cnp6.narm <- na.omit(cnp6)
dim(cnp6.narm)


```

<br/><br/>

#### linear relationship between each variable
- variable P4080 & P4079, P4080 & P4078, P4079 & P4075 seems to have linear relationship.
- Whereas P4079& p4077, p4080 & p4076 seems to have almost no linear relationship(random relationship). 

```{r}
library(ggplot2)
panel.lm <- function(x, y, col=par("col"), bg=NA, pch=par("pch"), 
                     cex=1, col.smooth="black", ...) {
  points(x, y, pch=20, col=alpha("midnightblue", 0.2), bg=bg, cex=cex) 
  abline(stats::lm(y~x), col=col.smooth, ...)
} 

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(.5, 0.5, txt, cex = cex.cor * r)
} 


panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "darkblue", ...)
} 


pairs(cnp6.narm[2:7], 
      lower.panel = panel.lm, 
      upper.panel = panel.cor,
      diag.panel = panel.hist,
      pch=20,
      main = "scatter-plot matrix, correlation coef., histogram")


```


#### Let'do PCA anlaysis.  -> doenst seem to be significant
```{r}
pr.out <- prcomp(cnp6.narm[2:7], scale=TRUE)
Cols <- function(vec) {
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}
```

# 2. Hierarchichal Clustering

#### Which method fits best for our data? 
- complete method calculated with maximum distance matrix!
```{r}
library(NbClust)
library(factoextra)

cnp.scaled <- cbind(cnp6.narm$RowNames, scale(cnp6[, 2:7]))
# cnp.dist.man <- dist(cnp.scaled, method = "manhattan")
# cnp.dist.euc <- dist(cnp.scaled, method = "euclidean")
# cnp.dist.can <- dist(cnp.scaled, method = "canberra")
cnp.dist.max <- dist(cnp.scaled, method = "maximum")
# cnp.dist.mink <- dist(cnp.scaled, method = "minkowski")
# 
# cnp.hclust.man <- hclust(cnp.dist.man, method = "average")
# cnp.hclust.euc <- hclust(cnp.dist.euc, method = "average")
# cnp.hclust.can <- hclust(cnp.dist.can, method = "average")
# cnp.hclust.max1 <- hclust(cnp.dist.max, method = "average") #less good
cnp.hclust.max2 <- hclust(cnp.dist.max, method = "complete")
# cnp.hclust.mink <- hclust(cnp.dist.mink, method = "average")
# summary(cnp.hclust.man)
# summary(cnp.hclust.euc)
# summary(cnp.hclust.can)
summary(cnp.hclust.max2)
# summary(cnp.hclust.mink)
# 
# 
# par(mfrow=c(2,2))
# 
# plot(cnp.hclust.euc)
# plot(cnp.hclust.man)
# plot(cnp.hclust.can)
# plot(cnp.hclust.euc)
# plot(cnp.hclust.max1)
plot(cnp.hclust.max2)
```


####  Let's visualize the result. 
- **number of clusters** : 4
- **distance method** : maximum
- **clustering method** : complete
```{r}
library(RColorBrewer)
fviz_dend(
  cnp.hclust.max2, k = 4, cex = 0.3, 
  k_colors = brewer.pal(4, "Set1"),
  color_labels_by_k = TRUE, rect = TRUE
)

hc.class <- cutree(cnp.hclust.max2, k = 4)
table(hc.class)
plot(table(hc.class))
```

# 3. Determining number of clusters
```{r, eval=FALSE}
# Elbow mehod  -----------------------------------------------------------------
fviz_nbclust(cnp.scaled.narm, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2) +
  labs(subtitle = "Elbow method")

# Silhouette method ----------------------------------------------------------------------
fviz_nbclust(cnp.scaled.narm, kmeans, method = "silhouette") + # This stays 2clusters are optimal cluster.
  labs(subtitle = "Silhouette method")

# Gap stats ----------------------------------------------------------------
nboot <- 50 # to keep the function speedy. recommended value:
# nboot= 500 for your analysis. Use verbose = FALSE to hide computing progression.
set.seed(123) # for reproduction. we set seeds.
fviz_nbclust(
  cnp.scaled, kmeans,
  nstart = 25, method = "gap_stat",
  nboot = 50
) +
  labs(subtitle = "Gap statistic method")
# bar represents   1 standard deviation


# determine which clusterign method is good  ------------------------------------------------------------
```


- concerning Euclidean distance, maximum distance ands canberaa distance each, it seems that dividing the data into 4 groups seems to be the best.
```{r}
# 몇 개의 그룹으로 나누는 것이 최적일지 알아보자.
library("NbClust")
nb <- NbClust(
  df, distance = "euclidean",
  min.nc = 2, max.nc = 10, method = "kmeans"
)

nb <- NbClust(
  df, distance = "maximum",
  min.nc = 2, max.nc = 10, method = "kmeans"
)

nb <- NbClust(
  df, distance = "canberra",
  min.nc = 2, max.nc = 10, method = "kmeans"
)



nbc <- fviz_nbclust(nb)
nbc
```

# 예제 풀어보기
```{r}
library(ISLR)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)

```

PCA를　한번　해보자．　추　clustering의　결과와　얼마나　맞닿는지　확인할　것이다．　
```{r}
pr.out <- prcomp(nci.data, scale=TRUE)
Cols <- function(vec) {
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

par(mfrow=c(1,2))

# pca 분석한 것을 질병 유형별로 볼 수 있다. 
plot(pr.out$x[,1:2],
     col=Cols(nci.labs),
     pch=19,
     xlab='Z1',
     ylab='Z2')
plot(pr.out$x[,1:2]) # 단순pca 분석
plot(pr.out$x[,c(1,3)]) #단순 pca 분석
plot(pr.out$x[,c(1,3)],
     col=Cols(nci.labs),
     pch=19,
     xlab='Z1',
     ylab='Z2')

summary(pr.out)



```



```{r}
plot(pr.out)

```

#Mixture model
```{r}
library(mclust)
fit <- Mclust(USArrests)
plot(fit, what='classification')
fit$classification

```

석