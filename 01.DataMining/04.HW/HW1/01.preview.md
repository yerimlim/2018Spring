R Notebook
================

copy number 어떤 사람은 유전자에 삭제된 정보도 있고 어떤 사람은 똑같은게 copy된게 많기도 하다. 얼마나 많은 copy가 있는지, 매칭해서 확인한다. (intensity를 log로 변환해서 분석한다) 측정하는 부분을 probe로 나누어서 본다.

copy number가 몇 개 있는지... probe가 6개 있다.

데이터 정ㅈ
===========

``` r
cnp6 <- read.csv("./cnp6.csv")
head(cnp6)
```

    ##   RowNames p4075 p4076 p4077 p4078 p4079 p4080
    ## 1   BA0485  1.26  1.07  1.31  0.78  0.97  1.11
    ## 2   BA0493  1.07  0.64  0.81  1.13  1.08  1.48
    ## 3   BA0517  0.93  0.89  1.13  1.02  1.04  1.23
    ## 4   BA0523  0.93  0.96  1.16  0.82  0.84  1.17
    ## 5   BA0543  1.17  0.87  0.74  0.99  0.96  0.86
    ## 6   BA0559  0.72  0.87  1.07  0.99  1.08  1.14

``` r
summary(cnp6)
```

    ##     RowNames       p4075           p4076           p4077      
    ##  BA0227 :  1   Min.   :0.440   Min.   :0.560   Min.   :0.590  
    ##  BA0229 :  1   1st Qu.:0.950   1st Qu.:0.970   1st Qu.:0.980  
    ##  BA0267 :  1   Median :1.040   Median :1.090   Median :1.120  
    ##  BA0269 :  1   Mean   :1.057   Mean   :1.102   Mean   :1.136  
    ##  BA0271 :  1   3rd Qu.:1.150   3rd Qu.:1.210   3rd Qu.:1.270  
    ##  BA0273 :  1   Max.   :2.150   Max.   :2.010   Max.   :2.240  
    ##  (Other):399   NA's   :2       NA's   :2       NA's   :2      
    ##      p4078           p4079            p4080      
    ##  Min.   :0.650   Min.   :0.6600   Min.   :0.660  
    ##  1st Qu.:0.930   1st Qu.:0.8900   1st Qu.:0.980  
    ##  Median :1.000   Median :0.9800   Median :1.090  
    ##  Mean   :1.011   Mean   :0.9888   Mean   :1.111  
    ##  3rd Qu.:1.080   3rd Qu.:1.0700   3rd Qu.:1.230  
    ##  Max.   :1.580   Max.   :1.7200   Max.   :1.820  
    ##  NA's   :2       NA's   :2        NA's   :2

``` r
str(cnp6)
```

    ## 'data.frame':    405 obs. of  7 variables:
    ##  $ RowNames: Factor w/ 405 levels "BA0227","BA0229",..: 39 40 41 42 43 44 45 46 275 276 ...
    ##  $ p4075   : num  1.26 1.07 0.93 0.93 1.17 0.72 0.94 0.92 0.78 0.96 ...
    ##  $ p4076   : num  1.07 0.64 0.89 0.96 0.87 0.87 0.86 0.95 1.27 0.85 ...
    ##  $ p4077   : num  1.31 0.81 1.13 1.16 0.74 1.07 1.26 1.03 0.93 0.94 ...
    ##  $ p4078   : num  0.78 1.13 1.02 0.82 0.99 0.99 1.06 1.03 0.78 1.17 ...
    ##  $ p4079   : num  0.97 1.08 1.04 0.84 0.96 1.08 1.03 1 0.86 1.06 ...
    ##  $ p4080   : num  1.11 1.48 1.23 1.17 0.86 1.14 1.17 1.18 0.98 0.89 ...

``` r
# 결측치를 찾아내자.
p1 <- cnp6[is.na(cnp6$p4075), ]
p2 <- cnp6[is.na(cnp6$p4076), ]
p3 <- cnp6[is.na(cnp6$p4077), ]
p4 <- cnp6[is.na(cnp6$p4078), ]
p5 <- cnp6[is.na(cnp6$p4079), ]
p6 <- cnp6[is.na(cnp6$p4080), ]
p1
```

    ##     RowNames p4075 p4076 p4077 p4078 p4079 p4080
    ## 286   DH0722    NA    NA    NA    NA    NA    NA
    ## 357   JL0776    NA    NA    NA    NA    NA    NA

``` r
p2
```

    ##     RowNames p4075 p4076 p4077 p4078 p4079 p4080
    ## 286   DH0722    NA    NA    NA    NA    NA    NA
    ## 357   JL0776    NA    NA    NA    NA    NA    NA

``` r
p3
```

    ##     RowNames p4075 p4076 p4077 p4078 p4079 p4080
    ## 286   DH0722    NA    NA    NA    NA    NA    NA
    ## 357   JL0776    NA    NA    NA    NA    NA    NA

``` r
p4
```

    ##     RowNames p4075 p4076 p4077 p4078 p4079 p4080
    ## 286   DH0722    NA    NA    NA    NA    NA    NA
    ## 357   JL0776    NA    NA    NA    NA    NA    NA

``` r
p5
```

    ##     RowNames p4075 p4076 p4077 p4078 p4079 p4080
    ## 286   DH0722    NA    NA    NA    NA    NA    NA
    ## 357   JL0776    NA    NA    NA    NA    NA    NA

``` r
p6
```

    ##     RowNames p4075 p4076 p4077 p4078 p4079 p4080
    ## 286   DH0722    NA    NA    NA    NA    NA    NA
    ## 357   JL0776    NA    NA    NA    NA    NA    NA

``` r
p1$RowNames == p2$RowNames
```

    ## [1] TRUE TRUE

``` r
p2$RowNames == p3$RowNames
```

    ## [1] TRUE TRUE

``` r
p3$RowNames == p4$RowNames
```

    ## [1] TRUE TRUE

``` r
# 결측치를 제거해보자.
na.value <- cnp6$RowNames[is.na(cnp6$p4075)]
na.value # 어떤 행이 결측치를 가지는지 확인해보고, 해당 행을 제거한다.
```

    ## [1] DH0722 JL0776
    ## 405 Levels: BA0227 BA0229 BA0267 BA0269 BA0271 BA0273 BA0275 ... SM0671

``` r
cnp6.narm <- cnp6[!is.na(cnp6$p4075), ]
summary(cnp6.narm)
```

    ##     RowNames       p4075           p4076           p4077      
    ##  BA0227 :  1   Min.   :0.440   Min.   :0.560   Min.   :0.590  
    ##  BA0229 :  1   1st Qu.:0.950   1st Qu.:0.970   1st Qu.:0.980  
    ##  BA0267 :  1   Median :1.040   Median :1.090   Median :1.120  
    ##  BA0269 :  1   Mean   :1.057   Mean   :1.102   Mean   :1.136  
    ##  BA0271 :  1   3rd Qu.:1.150   3rd Qu.:1.210   3rd Qu.:1.270  
    ##  BA0273 :  1   Max.   :2.150   Max.   :2.010   Max.   :2.240  
    ##  (Other):397                                                  
    ##      p4078           p4079            p4080      
    ##  Min.   :0.650   Min.   :0.6600   Min.   :0.660  
    ##  1st Qu.:0.930   1st Qu.:0.8900   1st Qu.:0.980  
    ##  Median :1.000   Median :0.9800   Median :1.090  
    ##  Mean   :1.011   Mean   :0.9888   Mean   :1.111  
    ##  3rd Qu.:1.080   3rd Qu.:1.0700   3rd Qu.:1.230  
    ##  Max.   :1.580   Max.   :1.7200   Max.   :1.820  
    ## 

계층적 군집분서
===============

``` r
library(NbClust)
library(factoextra)
```

    ## Loading required package: ggplot2

    ## Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ

``` r
cnp.scaled <- rbind(cnp6$RowNames, scale(cnp6[, 2:7]))
```

    ## Warning in rbind(cnp6$RowNames, scale(cnp6[, 2:7])): number of columns of
    ## result is not a multiple of vector length (arg 1)

``` r
cnp.scaled.narm <- rbind(cnp6.narm$RowNames, scale(cnp6.narm[, 2:7]))
```

    ## Warning in rbind(cnp6.narm$RowNames, scale(cnp6.narm[, 2:7])): number of
    ## columns of result is not a multiple of vector length (arg 1)

``` r
str(cnp.scaled.narm)
```

    ##  num [1:404, 1:6] 39 1.1001 0.0689 -0.6908 -0.6908 ...
    ##  - attr(*, "dimnames")=List of 2
    ##   ..$ : chr [1:404] "" "1" "2" "3" ...
    ##   ..$ : chr [1:6] "p4075" "p4076" "p4077" "p4078" ...

``` r
summary(cnp.scaled.narm)
```

    ##      p4075              p4076              p4077         
    ##  Min.   :-3.35006   Min.   :-2.47869   Min.   :-2.27039  
    ##  1st Qu.:-0.58229   1st Qu.:-0.60464   1st Qu.:-0.64884  
    ##  Median :-0.09386   Median :-0.05614   Median :-0.06675  
    ##  Mean   : 0.09653   Mean   : 0.09901   Mean   : 0.10149  
    ##  3rd Qu.: 0.51668   3rd Qu.: 0.49236   3rd Qu.: 0.56731  
    ##  Max.   :39.00000   Max.   :40.00000   Max.   :41.00000  
    ##      p4078              p4079              p4080        
    ##  Min.   :-2.79802   Min.   :-2.28426   Min.   :-2.2920  
    ##  1st Qu.:-0.62821   1st Qu.:-0.68631   1st Qu.:-0.6642  
    ##  Median :-0.08576   Median :-0.06103   Median :-0.1046  
    ##  Mean   : 0.10396   Mean   : 0.10644   Mean   : 0.1089  
    ##  3rd Qu.: 0.53418   3rd Qu.: 0.58162   3rd Qu.: 0.6075  
    ##  Max.   :42.00000   Max.   :43.00000   Max.   :44.0000

``` r
cnp.dist <- dist(cnp.scaled.narm, method = "manhattan")
cnp.hclust <- hclust(cnp.dist, method = "complete")
plot(cnp.hclust)
```

![](01.preview_files/figure-markdown_github/unnamed-chunk-2-1.png)

``` r
# fviz_dend(
#   cnp.hclust, k = 4, cex = 0.3, k_colors = c("blue", "red", "grey", "yellow"),
#   color_labels_by_k = TRUE, rect = TRUE
# )

hc.class <- cutree(cnp.hclust, k = 4)

hist(hc.class, c(1, 1.5, 2.5, 3.5, 4))
```

![](01.preview_files/figure-markdown_github/unnamed-chunk-2-2.png)

적당한 그룹 개수 정하기
=======================

``` r
# Elbow mehod  -----------------------------------------------------------------
fviz_nbclust(cnp.scaled.narm, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2) +
  labs(subtitle = "Elbow method")

# Silhouette method ----------------------------------------------------------------------
fviz_nbclust(cnp.scaled.narm, kmeans, method = "silhouette") + # This stays 2clusters are optimal cluster.
  labs(subtitle = "Silhouette method")

# Gap stats ----------------------------------------------------------------
nboot <- 50 # to keep the function speedy. recommended value:
# nboot= 500 for your analysis. Use verbose = FALSE to hide computing progression.
set.seed(123) # for reproduction. we set seeds.
fviz_nbclust(
  cnp.scaled.narm, kmeans,
  nstart = 25, method = "gap_stat",
  nboot = 50
) +
  labs(subtitle = "Gap statistic method")
# bar represents   1 standard deviation


# determine which clusterign method is good  ------------------------------------------------------------

# 몇 개의 그룹으로 나누는 것이 최적일지 알아보자.
library("NbClust")
nb <- NbClust(
  df, distance = "euclidean",
  min.nc = 2, max.nc = 10, method = "kmeans"
)

nbc <- fviz_nbclust(nb)
nbc
```

예제 풀어보기
=============

``` r
library(ISLR)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
```

    ## [1]   64 6830

``` r
nci.labs[1:4]
```

    ## [1] "CNS"   "CNS"   "CNS"   "RENAL"

``` r
table(nci.labs)
```

    ## nci.labs
    ##      BREAST         CNS       COLON K562A-repro K562B-repro    LEUKEMIA 
    ##           7           5           7           1           1           6 
    ## MCF7A-repro MCF7D-repro    MELANOMA       NSCLC     OVARIAN    PROSTATE 
    ##           1           1           8           9           6           2 
    ##       RENAL     UNKNOWN 
    ##           9           1

PCA를　한번　해보자．　추　clustering의　결과와　얼마나　맞닿는지　확인할　것이다．　

``` r
pr.out <- prcomp(nci.data, scale=TRUE)
Cols <- function(vec) {
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}

par(mfrow=c(1,2))

# pca 분석한 것을 질병 유형별로 볼 수 있다. 
plot(pr.out$x[,1:2],
     col=Cols(nci.labs),
     pch=19,
     xlab='Z1',
     ylab='Z2')
plot(pr.out$x[,1:2]) # 단순pca 분석
```

![](01.preview_files/figure-markdown_github/unnamed-chunk-5-1.png)

``` r
plot(pr.out$x[,c(1,3)]) #단순 pca 분석
plot(pr.out$x[,c(1,3)],
     col=Cols(nci.labs),
     pch=19,
     xlab='Z1',
     ylab='Z2')
```

![](01.preview_files/figure-markdown_github/unnamed-chunk-5-2.png)

``` r
summary(pr.out)
```

    ## Importance of components:
    ##                            PC1      PC2      PC3      PC4      PC5
    ## Standard deviation     27.8535 21.48136 19.82046 17.03256 15.97181
    ## Proportion of Variance  0.1136  0.06756  0.05752  0.04248  0.03735
    ## Cumulative Proportion   0.1136  0.18115  0.23867  0.28115  0.31850
    ##                             PC6      PC7      PC8      PC9     PC10
    ## Standard deviation     15.72108 14.47145 13.54427 13.14400 12.73860
    ## Proportion of Variance  0.03619  0.03066  0.02686  0.02529  0.02376
    ## Cumulative Proportion   0.35468  0.38534  0.41220  0.43750  0.46126
    ##                            PC11     PC12     PC13     PC14     PC15
    ## Standard deviation     12.68672 12.15769 11.83019 11.62554 11.43779
    ## Proportion of Variance  0.02357  0.02164  0.02049  0.01979  0.01915
    ## Cumulative Proportion   0.48482  0.50646  0.52695  0.54674  0.56590
    ##                            PC16     PC17     PC18     PC19    PC20
    ## Standard deviation     11.00051 10.65666 10.48880 10.43518 10.3219
    ## Proportion of Variance  0.01772  0.01663  0.01611  0.01594  0.0156
    ## Cumulative Proportion   0.58361  0.60024  0.61635  0.63229  0.6479
    ##                            PC21    PC22    PC23    PC24    PC25    PC26
    ## Standard deviation     10.14608 10.0544 9.90265 9.64766 9.50764 9.33253
    ## Proportion of Variance  0.01507  0.0148 0.01436 0.01363 0.01324 0.01275
    ## Cumulative Proportion   0.66296  0.6778 0.69212 0.70575 0.71899 0.73174
    ##                           PC27   PC28    PC29    PC30    PC31    PC32
    ## Standard deviation     9.27320 9.0900 8.98117 8.75003 8.59962 8.44738
    ## Proportion of Variance 0.01259 0.0121 0.01181 0.01121 0.01083 0.01045
    ## Cumulative Proportion  0.74433 0.7564 0.76824 0.77945 0.79027 0.80072
    ##                           PC33    PC34    PC35    PC36    PC37    PC38
    ## Standard deviation     8.37305 8.21579 8.15731 7.97465 7.90446 7.82127
    ## Proportion of Variance 0.01026 0.00988 0.00974 0.00931 0.00915 0.00896
    ## Cumulative Proportion  0.81099 0.82087 0.83061 0.83992 0.84907 0.85803
    ##                           PC39    PC40    PC41   PC42    PC43   PC44
    ## Standard deviation     7.72156 7.58603 7.45619 7.3444 7.10449 7.0131
    ## Proportion of Variance 0.00873 0.00843 0.00814 0.0079 0.00739 0.0072
    ## Cumulative Proportion  0.86676 0.87518 0.88332 0.8912 0.89861 0.9058
    ##                           PC45   PC46    PC47    PC48    PC49    PC50
    ## Standard deviation     6.95839 6.8663 6.80744 6.64763 6.61607 6.40793
    ## Proportion of Variance 0.00709 0.0069 0.00678 0.00647 0.00641 0.00601
    ## Cumulative Proportion  0.91290 0.9198 0.92659 0.93306 0.93947 0.94548
    ##                           PC51    PC52    PC53    PC54    PC55    PC56
    ## Standard deviation     6.21984 6.20326 6.06706 5.91805 5.91233 5.73539
    ## Proportion of Variance 0.00566 0.00563 0.00539 0.00513 0.00512 0.00482
    ## Cumulative Proportion  0.95114 0.95678 0.96216 0.96729 0.97241 0.97723
    ##                           PC57   PC58    PC59    PC60    PC61    PC62
    ## Standard deviation     5.47261 5.2921 5.02117 4.68398 4.17567 4.08212
    ## Proportion of Variance 0.00438 0.0041 0.00369 0.00321 0.00255 0.00244
    ## Cumulative Proportion  0.98161 0.9857 0.98940 0.99262 0.99517 0.99761
    ##                           PC63      PC64
    ## Standard deviation     4.04124 2.148e-14
    ## Proportion of Variance 0.00239 0.000e+00
    ## Cumulative Proportion  1.00000 1.000e+00

``` r
plot(pr.out)
```

![](01.preview_files/figure-markdown_github/unnamed-chunk-6-1.png)

Mixture model
=============

``` r
library(mclust)
```

    ## Package 'mclust' version 5.4
    ## Type 'citation("mclust")' for citing this R package in publications.

``` r
fit <- Mclust(USArrests)
plot(fit, what='classification')
```

![](01.preview_files/figure-markdown_github/unnamed-chunk-7-1.png)

``` r
fit$classification
```

    ##        Alabama         Alaska        Arizona       Arkansas     California 
    ##              1              1              1              3              1 
    ##       Colorado    Connecticut       Delaware        Florida        Georgia 
    ##              1              3              3              1              1 
    ##         Hawaii          Idaho       Illinois        Indiana           Iowa 
    ##              3              2              1              3              2 
    ##         Kansas       Kentucky      Louisiana          Maine       Maryland 
    ##              3              3              1              2              1 
    ##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri 
    ##              3              1              2              1              1 
    ##        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
    ##              3              3              1              2              3 
    ##     New Mexico       New York North Carolina   North Dakota           Ohio 
    ##              1              1              1              2              3 
    ##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
    ##              3              3              3              3              1 
    ##   South Dakota      Tennessee          Texas           Utah        Vermont 
    ##              2              1              1              3              2 
    ##       Virginia     Washington  West Virginia      Wisconsin        Wyoming 
    ##              3              3              2              2              3

석
